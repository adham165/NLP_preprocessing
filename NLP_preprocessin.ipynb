{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSodiJ8901x5",
        "outputId": "dce6c5c9-d4b6-4d26-ad1f-291326698e5d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('maxent_ne_chunker_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ye8UfGgz0sMy",
        "outputId": "233220a3-9391-4773-b6e1-02519a9555d1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b61cf9bb",
        "outputId": "3e2ee009-d945-421c-d7c6-d640d3b58ff2"
      },
      "source": [
        "!ls -l /root/nltk_data/chunkers/maxent_ne_chunker_tab/english_ace_multiclass/"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 7404\n",
            "-rw-r--r-- 1 root root     208 Sep 17 18:14 alwayson.tab\n",
            "-rw-r--r-- 1 root root     117 Sep 17 18:14 labels.txt\n",
            "-rw-r--r-- 1 root root 4366389 Sep 17 18:14 mapping.tab\n",
            "-rw-r--r-- 1 root root 3200577 Sep 17 18:14 weights.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "do_1KMJE0d-x"
      },
      "outputs": [],
      "source": [
        "text_data = [\n",
        "    \"The movie was fantastic and I loved every part of it about Egypt\",\n",
        "    \"I hated the film, it was the worst I have ever seen\",\n",
        "    \"The storyline was boring but the acting was brilliant\",\n",
        "    \"An amazing movie with a great plot and incredible performances\",\n",
        "    \"Egypt movie, I regret wasting my time on it\",\n",
        "    \"The actors did a great job but the story lacked depth\",\n",
        "    \"One of the best films I have seen in a long time, highly recommend it\",\n",
        "    \"This film was just okay, not too bad but not great either\",\n",
        "    \"Absolutely loved the movie, fantastic plot and wonderful cast\",\n",
        "    \"The movie was disappointing, it did not live up to the hype\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data=[]\n",
        "for text in text_data:\n",
        "  tokenized_data.append(nltk.tokenize.word_tokenize(text))"
      ],
      "metadata": {
        "id": "f8pAJoZW0hOh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "tokenized_data_without_stopwords = []\n",
        "for tokens in tokenized_data:\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "    tokenized_data_without_stopwords.append(filtered_tokens)\n",
        "tokenized_data_without_stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyyVmc7M1IuR",
        "outputId": "1d86d955-e25c-4eaf-cf88-7499a99fd13a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['movie', 'fantastic', 'loved', 'every', 'part', 'Egypt'],\n",
              " ['hated', 'film', ',', 'worst', 'ever', 'seen'],\n",
              " ['storyline', 'boring', 'acting', 'brilliant'],\n",
              " ['amazing', 'movie', 'great', 'plot', 'incredible', 'performances'],\n",
              " ['Egypt', 'movie', ',', 'regret', 'wasting', 'time'],\n",
              " ['actors', 'great', 'job', 'story', 'lacked', 'depth'],\n",
              " ['One', 'best', 'films', 'seen', 'long', 'time', ',', 'highly', 'recommend'],\n",
              " ['film', 'okay', ',', 'bad', 'great', 'either'],\n",
              " ['Absolutely',\n",
              "  'loved',\n",
              "  'movie',\n",
              "  ',',\n",
              "  'fantastic',\n",
              "  'plot',\n",
              "  'wonderful',\n",
              "  'cast'],\n",
              " ['movie', 'disappointing', ',', 'live', 'hype']]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_data = []\n",
        "for tokens in tokenized_data_without_stopwords:\n",
        "    stemmed_tokens = [nltk.stem.PorterStemmer().stem(word) for word in tokens]\n",
        "    stemmed_data.append(stemmed_tokens)\n",
        "stemmed_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nwq62Kr-1txp",
        "outputId": "85da330f-cf63-48d3-ea4a-d49a614d772d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['movi', 'fantast', 'love', 'everi', 'part', 'egypt'],\n",
              " ['hate', 'film', ',', 'worst', 'ever', 'seen'],\n",
              " ['storylin', 'bore', 'act', 'brilliant'],\n",
              " ['amaz', 'movi', 'great', 'plot', 'incred', 'perform'],\n",
              " ['egypt', 'movi', ',', 'regret', 'wast', 'time'],\n",
              " ['actor', 'great', 'job', 'stori', 'lack', 'depth'],\n",
              " ['one', 'best', 'film', 'seen', 'long', 'time', ',', 'highli', 'recommend'],\n",
              " ['film', 'okay', ',', 'bad', 'great', 'either'],\n",
              " ['absolut', 'love', 'movi', ',', 'fantast', 'plot', 'wonder', 'cast'],\n",
              " ['movi', 'disappoint', ',', 'live', 'hype']]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmetized_data = []\n",
        "for tokens in tokenized_data_without_stopwords:\n",
        "    lemmetized_tokens = [nltk.stem.WordNetLemmatizer().lemmatize(word) for word in tokens]\n",
        "    lemmetized_data.append(lemmetized_tokens)\n",
        "lemmetized_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcV83MZd2meK",
        "outputId": "3d805d6a-6e83-4049-c34d-91b11158cde2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['movie', 'fantastic', 'loved', 'every', 'part', 'Egypt'],\n",
              " ['hated', 'film', ',', 'worst', 'ever', 'seen'],\n",
              " ['storyline', 'boring', 'acting', 'brilliant'],\n",
              " ['amazing', 'movie', 'great', 'plot', 'incredible', 'performance'],\n",
              " ['Egypt', 'movie', ',', 'regret', 'wasting', 'time'],\n",
              " ['actor', 'great', 'job', 'story', 'lacked', 'depth'],\n",
              " ['One', 'best', 'film', 'seen', 'long', 'time', ',', 'highly', 'recommend'],\n",
              " ['film', 'okay', ',', 'bad', 'great', 'either'],\n",
              " ['Absolutely',\n",
              "  'loved',\n",
              "  'movie',\n",
              "  ',',\n",
              "  'fantastic',\n",
              "  'plot',\n",
              "  'wonderful',\n",
              "  'cast'],\n",
              " ['movie', 'disappointing', ',', 'live', 'hype']]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "POS=[]\n",
        "for data in tokenized_data:\n",
        "  POS.append(nltk.pos_tag(data))\n",
        "POS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwPo-kcL28kh",
        "outputId": "4f509998-73cf-4b5b-991d-abe38d97b0da"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('The', 'DT'),\n",
              "  ('movie', 'NN'),\n",
              "  ('was', 'VBD'),\n",
              "  ('fantastic', 'JJ'),\n",
              "  ('and', 'CC'),\n",
              "  ('I', 'PRP'),\n",
              "  ('loved', 'VBD'),\n",
              "  ('every', 'DT'),\n",
              "  ('part', 'NN'),\n",
              "  ('of', 'IN'),\n",
              "  ('it', 'PRP'),\n",
              "  ('about', 'IN'),\n",
              "  ('Egypt', 'NNP')],\n",
              " [('I', 'PRP'),\n",
              "  ('hated', 'VBD'),\n",
              "  ('the', 'DT'),\n",
              "  ('film', 'NN'),\n",
              "  (',', ','),\n",
              "  ('it', 'PRP'),\n",
              "  ('was', 'VBD'),\n",
              "  ('the', 'DT'),\n",
              "  ('worst', 'JJS'),\n",
              "  ('I', 'PRP'),\n",
              "  ('have', 'VBP'),\n",
              "  ('ever', 'RB'),\n",
              "  ('seen', 'VBN')],\n",
              " [('The', 'DT'),\n",
              "  ('storyline', 'NN'),\n",
              "  ('was', 'VBD'),\n",
              "  ('boring', 'VBG'),\n",
              "  ('but', 'CC'),\n",
              "  ('the', 'DT'),\n",
              "  ('acting', 'NN'),\n",
              "  ('was', 'VBD'),\n",
              "  ('brilliant', 'JJ')],\n",
              " [('An', 'DT'),\n",
              "  ('amazing', 'JJ'),\n",
              "  ('movie', 'NN'),\n",
              "  ('with', 'IN'),\n",
              "  ('a', 'DT'),\n",
              "  ('great', 'JJ'),\n",
              "  ('plot', 'NN'),\n",
              "  ('and', 'CC'),\n",
              "  ('incredible', 'JJ'),\n",
              "  ('performances', 'NNS')],\n",
              " [('Egypt', 'NNP'),\n",
              "  ('movie', 'NN'),\n",
              "  (',', ','),\n",
              "  ('I', 'PRP'),\n",
              "  ('regret', 'VBP'),\n",
              "  ('wasting', 'VBG'),\n",
              "  ('my', 'PRP$'),\n",
              "  ('time', 'NN'),\n",
              "  ('on', 'IN'),\n",
              "  ('it', 'PRP')],\n",
              " [('The', 'DT'),\n",
              "  ('actors', 'NNS'),\n",
              "  ('did', 'VBD'),\n",
              "  ('a', 'DT'),\n",
              "  ('great', 'JJ'),\n",
              "  ('job', 'NN'),\n",
              "  ('but', 'CC'),\n",
              "  ('the', 'DT'),\n",
              "  ('story', 'NN'),\n",
              "  ('lacked', 'VBD'),\n",
              "  ('depth', 'NN')],\n",
              " [('One', 'CD'),\n",
              "  ('of', 'IN'),\n",
              "  ('the', 'DT'),\n",
              "  ('best', 'JJS'),\n",
              "  ('films', 'NNS'),\n",
              "  ('I', 'PRP'),\n",
              "  ('have', 'VBP'),\n",
              "  ('seen', 'VBN'),\n",
              "  ('in', 'IN'),\n",
              "  ('a', 'DT'),\n",
              "  ('long', 'JJ'),\n",
              "  ('time', 'NN'),\n",
              "  (',', ','),\n",
              "  ('highly', 'RB'),\n",
              "  ('recommend', 'VB'),\n",
              "  ('it', 'PRP')],\n",
              " [('This', 'DT'),\n",
              "  ('film', 'NN'),\n",
              "  ('was', 'VBD'),\n",
              "  ('just', 'RB'),\n",
              "  ('okay', 'RB'),\n",
              "  (',', ','),\n",
              "  ('not', 'RB'),\n",
              "  ('too', 'RB'),\n",
              "  ('bad', 'JJ'),\n",
              "  ('but', 'CC'),\n",
              "  ('not', 'RB'),\n",
              "  ('great', 'JJ'),\n",
              "  ('either', 'CC')],\n",
              " [('Absolutely', 'RB'),\n",
              "  ('loved', 'VBD'),\n",
              "  ('the', 'DT'),\n",
              "  ('movie', 'NN'),\n",
              "  (',', ','),\n",
              "  ('fantastic', 'JJ'),\n",
              "  ('plot', 'NN'),\n",
              "  ('and', 'CC'),\n",
              "  ('wonderful', 'JJ'),\n",
              "  ('cast', 'NN')],\n",
              " [('The', 'DT'),\n",
              "  ('movie', 'NN'),\n",
              "  ('was', 'VBD'),\n",
              "  ('disappointing', 'JJ'),\n",
              "  (',', ','),\n",
              "  ('it', 'PRP'),\n",
              "  ('did', 'VBD'),\n",
              "  ('not', 'RB'),\n",
              "  ('live', 'VB'),\n",
              "  ('up', 'RB'),\n",
              "  ('to', 'TO'),\n",
              "  ('the', 'DT'),\n",
              "  ('hype', 'NN')]]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NER_data = []\n",
        "for tagged_tokens in POS:\n",
        "    ner_tree = nltk.ne_chunk(tagged_tokens)\n",
        "    NER_data.append(ner_tree)\n",
        "\n",
        "for i, tree in enumerate(NER_data):\n",
        "    print(f\"Sentence {i+1}:\")\n",
        "    print(tree)\n",
        "    print(\"-\" * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k33UjhXy4biY",
        "outputId": "184d7cda-a82c-43e0-c196-4614d65fe6d9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1:\n",
            "(S\n",
            "  The/DT\n",
            "  movie/NN\n",
            "  was/VBD\n",
            "  fantastic/JJ\n",
            "  and/CC\n",
            "  I/PRP\n",
            "  loved/VBD\n",
            "  every/DT\n",
            "  part/NN\n",
            "  of/IN\n",
            "  it/PRP\n",
            "  about/IN\n",
            "  (GPE Egypt/NNP))\n",
            "--------------------\n",
            "Sentence 2:\n",
            "(S\n",
            "  I/PRP\n",
            "  hated/VBD\n",
            "  the/DT\n",
            "  film/NN\n",
            "  ,/,\n",
            "  it/PRP\n",
            "  was/VBD\n",
            "  the/DT\n",
            "  worst/JJS\n",
            "  I/PRP\n",
            "  have/VBP\n",
            "  ever/RB\n",
            "  seen/VBN)\n",
            "--------------------\n",
            "Sentence 3:\n",
            "(S\n",
            "  The/DT\n",
            "  storyline/NN\n",
            "  was/VBD\n",
            "  boring/VBG\n",
            "  but/CC\n",
            "  the/DT\n",
            "  acting/NN\n",
            "  was/VBD\n",
            "  brilliant/JJ)\n",
            "--------------------\n",
            "Sentence 4:\n",
            "(S\n",
            "  An/DT\n",
            "  amazing/JJ\n",
            "  movie/NN\n",
            "  with/IN\n",
            "  a/DT\n",
            "  great/JJ\n",
            "  plot/NN\n",
            "  and/CC\n",
            "  incredible/JJ\n",
            "  performances/NNS)\n",
            "--------------------\n",
            "Sentence 5:\n",
            "(S\n",
            "  (GPE Egypt/NNP)\n",
            "  movie/NN\n",
            "  ,/,\n",
            "  I/PRP\n",
            "  regret/VBP\n",
            "  wasting/VBG\n",
            "  my/PRP$\n",
            "  time/NN\n",
            "  on/IN\n",
            "  it/PRP)\n",
            "--------------------\n",
            "Sentence 6:\n",
            "(S\n",
            "  The/DT\n",
            "  actors/NNS\n",
            "  did/VBD\n",
            "  a/DT\n",
            "  great/JJ\n",
            "  job/NN\n",
            "  but/CC\n",
            "  the/DT\n",
            "  story/NN\n",
            "  lacked/VBD\n",
            "  depth/NN)\n",
            "--------------------\n",
            "Sentence 7:\n",
            "(S\n",
            "  One/CD\n",
            "  of/IN\n",
            "  the/DT\n",
            "  best/JJS\n",
            "  films/NNS\n",
            "  I/PRP\n",
            "  have/VBP\n",
            "  seen/VBN\n",
            "  in/IN\n",
            "  a/DT\n",
            "  long/JJ\n",
            "  time/NN\n",
            "  ,/,\n",
            "  highly/RB\n",
            "  recommend/VB\n",
            "  it/PRP)\n",
            "--------------------\n",
            "Sentence 8:\n",
            "(S\n",
            "  This/DT\n",
            "  film/NN\n",
            "  was/VBD\n",
            "  just/RB\n",
            "  okay/RB\n",
            "  ,/,\n",
            "  not/RB\n",
            "  too/RB\n",
            "  bad/JJ\n",
            "  but/CC\n",
            "  not/RB\n",
            "  great/JJ\n",
            "  either/CC)\n",
            "--------------------\n",
            "Sentence 9:\n",
            "(S\n",
            "  Absolutely/RB\n",
            "  loved/VBD\n",
            "  the/DT\n",
            "  movie/NN\n",
            "  ,/,\n",
            "  fantastic/JJ\n",
            "  plot/NN\n",
            "  and/CC\n",
            "  wonderful/JJ\n",
            "  cast/NN)\n",
            "--------------------\n",
            "Sentence 10:\n",
            "(S\n",
            "  The/DT\n",
            "  movie/NN\n",
            "  was/VBD\n",
            "  disappointing/JJ\n",
            "  ,/,\n",
            "  it/PRP\n",
            "  did/VBD\n",
            "  not/RB\n",
            "  live/VB\n",
            "  up/RB\n",
            "  to/TO\n",
            "  the/DT\n",
            "  hype/NN)\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Ensure all lists have the same length (although they should based on the processing)\n",
        "min_len = min(len(tokenized_data_without_stopwords), len(lemmetized_data), len(stemmed_data))\n",
        "data_for_csv = {\n",
        "    'Tokens Without Stopwords': [' '.join(tokens) for tokens in tokenized_data_without_stopwords[:min_len]],\n",
        "    'Lemmatized Tokens': [' '.join(tokens) for tokens in lemmetized_data[:min_len]],\n",
        "    'Stemmed Tokens': [' '.join(tokens) for tokens in stemmed_data[:min_len]]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data_for_csv)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('processed_text_data.csv', index=False)\n",
        "\n",
        "print(\"Processed data saved to processed_text_data.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSonT-Qj5pQ5",
        "outputId": "fdc0e308-c05d-41fb-970c-dac173c7d701"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed data saved to processed_text_data.csv\n"
          ]
        }
      ]
    }
  ]
}